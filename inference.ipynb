{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "F0u7zLGfDGwh",
        "340I2HDd6Cwu",
        "kEQVNtEuKIVc",
        "ns_AOUFhDBVb",
        "qKw-e36NV6kh",
        "BfMIItLCCwAz",
        "7dn30IaVCbGK",
        "gtaiszzfCDhx",
        "LEirCq94B9h3"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading required files"
      ],
      "metadata": {
        "id": "F0u7zLGfDGwh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRS-KhFtgNCU",
        "outputId": "fb1564ac-2b66-4fdb-c640-498c1c0484f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18UdsemUdKo75GXmQLLVYdcOhNZ3zM215\n",
            "To: /content/shapenet_car.pt\n",
            "100% 518M/518M [00:02<00:00, 220MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 18UdsemUdKo75GXmQLLVYdcOhNZ3zM215"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q torch==1.11.0 torchvision==0.10.0 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install -q ninja xatlas gdown\n",
        "!pip install -q git+https://github.com/NVlabs/nvdiffrast/\n",
        "!pip install -q meshzoo ipdb imageio gputil h5py point-cloud-utils imageio imageio-ffmpeg==0.4.4 pyspng==0.1.0\n",
        "!pip install -q urllib3\n",
        "!pip install -q scipy\n",
        "!pip install -q click\n",
        "!pip install -q tqdm\n",
        "!pip install -q opencv-python==4.5.4.58"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz82uLcNgRTC",
        "outputId": "37ae2394-c4aa-4fab-a2ca-e728572fa33c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.4/229.4 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nvdiffrast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.2/106.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pyspng \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pyspng (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for pyspng\u001b[0m\u001b[31m\n",
            "\u001b[0m  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not build wheels for pyspng, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/m22cs058/GET3D.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sF6iQ7VgYjE",
        "outputId": "ad655171-2eb8-44b1-c9e7-11562ceaf379"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GET3D'...\n",
            "remote: Enumerating objects: 252, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 252 (delta 53), reused 48 (delta 40), pack-reused 136\u001b[K\n",
            "Receiving objects: 100% (252/252), 175.43 MiB | 26.64 MiB/s, done.\n",
            "Resolving deltas: 100% (76/76), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv shapenet_car.pt GET3D/pretrained_model/"
      ],
      "metadata": {
        "id": "CpOeJLbNg3FS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd GET3D"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZx74xj6g9EG",
        "outputId": "fa48685f-77d8-4d59-d4bc-b9633c590304"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GET3D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "340I2HDd6Cwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import click\n",
        "import re\n",
        "import json\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import torch\n",
        "import PIL.Image\n",
        "import imageio\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import dnnlib\n",
        "from torch_utils.ops import conv2d_gradfix, grid_sample_gradfix\n",
        "from training import training_loop_3d\n",
        "from metrics import metric_main\n",
        "from torch_utils import training_stats\n",
        "from torch_utils import custom_ops"
      ],
      "metadata": {
        "id": "iLmpcNJ66CA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and Saving Mesh and OBJs"
      ],
      "metadata": {
        "id": "kEQVNtEuKIVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_obj(points, faces, filepath):\n",
        "    with open(filepath, 'w') as f:\n",
        "        for point in points:\n",
        "            f.write(f\"v {point[0]} {point[1]} {point[2]}\\n\")\n",
        "\n",
        "        for face in faces:\n",
        "            f.write(f\"f {' '.join(str(index+1) for index in face)}\\n\")"
      ],
      "metadata": {
        "id": "huYroU5u_4PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def savemeshtes2(points, textures, faces, texture_faces, filepath):\n",
        "    folder, filename = os.path.split(filepath)\n",
        "    filename, _ = os.path.splitext(filename)\n",
        "\n",
        "    material_file = os.path.join(folder, f\"{filename}.mtl\")\n",
        "    with open(material_file, 'w') as f:\n",
        "        f.write(\"newmtl material_0\\n\")\n",
        "        f.write(\"map_Kd {}.png\\n\".format(filename))\n",
        "\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(f\"mtllib {filename}.mtl\\n\")\n",
        "\n",
        "        for point in points:\n",
        "            f.write(f\"v {point[0]} {point[1]} {point[2]}\\n\")\n",
        "\n",
        "        for texture in textures:\n",
        "            f.write(f\"vt {texture[0]} {texture[1]}\\n\")\n",
        "\n",
        "        f.write(\"usemtl material_0\\n\")\n",
        "\n",
        "        for face, tface in zip(faces, texture_faces):\n",
        "            f.write(f\"f {'/'.join(str(vidx) for vidx in [*face+1, *tface+1])}\\n\")"
      ],
      "metadata": {
        "id": "pd7gB6lg_6SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadobj(filepath):\n",
        "    verts = []\n",
        "    faces = []\n",
        "\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            tokens = line.strip().split()\n",
        "            if not tokens:\n",
        "                continue\n",
        "\n",
        "            if tokens[0] == 'v':\n",
        "                verts.append([float(t) for t in tokens[1:]] )\n",
        "\n",
        "            elif tokens[0] == 'f':\n",
        "                faces.append([int(t.split('/')[0]) - 1 for t in tokens[1:]])\n",
        "\n",
        "    faces = np.array(faces, dtype=np.int64)\n",
        "    verts = np.array(verts, dtype=np.float32)\n",
        "\n",
        "    return verts, faces\n",
        "\n",
        "\n",
        "def loadobjtex(filepath):\n",
        "    verts = []\n",
        "    textures = []\n",
        "    faces = []\n",
        "    texture_faces = []\n",
        "\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            tokens = line.strip().split()\n",
        "            if not tokens:\n",
        "                continue\n",
        "\n",
        "            if tokens[0] == 'v':\n",
        "                verts.append([float(t) for t in tokens[1:]])\n",
        "\n",
        "            elif tokens[0] == 'vt':\n",
        "                textures.append([float(t) for t in tokens[1:3]])\n",
        "\n",
        "            elif tokens[0] == 'f':\n",
        "                face_tokens = [t.split('/') for t in tokens[1:]]\n",
        "                face = [int(t[0]) - 1 for t in face_tokens]\n",
        "                tface = [int(t[1]) - 1 for t in face_tokens]\n",
        "\n",
        "                faces.append(face)\n",
        "                texture_faces.append(tface)\n",
        "\n",
        "    faces = np.array(faces, dtype=np.int64)\n",
        "    texture_faces = np.array(texture_faces, dtype=np.int64)\n",
        "    verts = np.array(verts, dtype=np.float32)\n",
        "    textures = np.array(textures, dtype=np.float32)\n",
        "\n",
        "    return verts, textures, faces, texture_faces"
      ],
      "metadata": {
        "id": "ZXKkmzHmHmwf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Util functions for inferencing and results\n",
        "\n"
      ],
      "metadata": {
        "id": "ns_AOUFhDBVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_image_grid(img, fname, drange, grid_size):\n",
        "    lo, hi = drange\n",
        "    img = np.asarray(img, dtype=np.float32)\n",
        "    img = (img - lo) * (255 / (hi - lo))\n",
        "    img = np.rint(img).clip(0, 255).astype(np.uint8)\n",
        "\n",
        "    gw, gh = grid_size\n",
        "    _N, C, H, W = img.shape\n",
        "    gw = _N // gh\n",
        "    img = img.reshape([gh, gw, C, H, W])\n",
        "    img = img.transpose(0, 3, 1, 4, 2)\n",
        "    img = img.reshape([gh * H, gw * W, C])\n",
        "\n",
        "    assert C in [1, 3]\n",
        "    if not fname is None:\n",
        "        if C == 1:\n",
        "            PIL.Image.fromarray(img[:, :, 0], 'L').save(fname)\n",
        "        if C == 3:\n",
        "            PIL.Image.fromarray(img, 'RGB').save(fname)\n",
        "    return img"
      ],
      "metadata": {
        "id": "bftcmxzeAJ96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_3d_shape(mesh_v_list, mesh_f_list, root, idx):\n",
        "    n_mesh = len(mesh_f_list)\n",
        "    mesh_dir = os.path.join(root, 'mesh_pred')\n",
        "    os.makedirs(mesh_dir, exist_ok=True)\n",
        "    for i_mesh in range(n_mesh):\n",
        "        mesh_v = mesh_v_list[i_mesh]\n",
        "        mesh_f = mesh_f_list[i_mesh]\n",
        "        mesh_name = os.path.join(mesh_dir, '%07d_%02d.obj' % (idx, i_mesh))\n",
        "        save_obj(mesh_v, mesh_f, mesh_name)\n"
      ],
      "metadata": {
        "id": "t2Yi0SXoAKI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_visualization_for_interpolation(generator, num_sam=10, c_to_compute_w_avg=None, save_dir=None, gen_mesh=False):\n",
        "    with torch.no_grad():\n",
        "        generator.update_w_avg(c_to_compute_w_avg)\n",
        "        geo_codes = torch.randn(num_sam, generator.z_dim, device=generator.device)\n",
        "        tex_codes = torch.randn(num_sam, generator.z_dim, device=generator.device)\n",
        "        ws_geo = generator.mapping_geo(geo_codes, None, truncation_psi=0.7)\n",
        "        ws_tex = generator.mapping(tex_codes, None, truncation_psi=0.7)\n",
        "        camera_list = [generator.synthesis.generate_rotate_camera_list(n_batch=num_sam)[4]]\n",
        "\n",
        "        select_geo_codes = np.arange(4)  # You can change to other selected shapes\n",
        "        select_tex_codes = np.arange(4)\n",
        "        for i in range(len(select_geo_codes) - 1):\n",
        "            ws_geo_a = ws_geo[select_geo_codes[i]].unsqueeze(dim=0)\n",
        "            ws_geo_b = ws_geo[select_geo_codes[i + 1]].unsqueeze(dim=0)\n",
        "            ws_tex_a = ws_tex[select_tex_codes[i]].unsqueeze(dim=0)\n",
        "            ws_tex_b = ws_tex[select_tex_codes[i + 1]].unsqueeze(dim=0)\n",
        "            new_ws_geo = []\n",
        "            new_ws_tex = []\n",
        "            n_interpolate = 10\n",
        "            for _i in range(n_interpolate):\n",
        "                w = float(_i + 1) / n_interpolate\n",
        "                w = 1 - w\n",
        "                new_ws_geo.append(ws_geo_a * w + ws_geo_b * (1 - w))\n",
        "                new_ws_tex.append(ws_tex_a * w + ws_tex_b * (1 - w))\n",
        "            new_ws_tex = torch.cat(new_ws_tex, dim=0)\n",
        "            new_ws_geo = torch.cat(new_ws_geo, dim=0)\n",
        "            save_path = os.path.join(save_dir, 'interpolate_%02d' % (i))\n",
        "            os.makedirs(save_path, exist_ok=True)\n",
        "            gen_swap(new_ws_geo, new_ws_tex, camera_list[0], generator, save_path=save_path, gen_mesh=gen_mesh)"
      ],
      "metadata": {
        "id": "T8th1xU6APro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_visualization(G_ema, grid_z, grid_c, run_dir, cur_nimg, grid_size, cur_tick, image_snapshot_ticks=50, save_gif_name=None, save_all=True, grid_tex_z=None,):\n",
        "    with torch.no_grad():\n",
        "        G_ema.update_w_avg()\n",
        "        camera_list = G_ema.synthesis.generate_rotate_camera_list(n_batch=grid_z[0].shape[0])\n",
        "        camera_img_list = []\n",
        "        if not save_all:\n",
        "            camera_list = [camera_list[4]]  # we only save one camera for this\n",
        "        if grid_tex_z is None:\n",
        "            grid_tex_z = grid_z\n",
        "        for i_camera, camera in enumerate(camera_list):\n",
        "            images_list = []\n",
        "            mesh_v_list = []\n",
        "            mesh_f_list = []\n",
        "            for z, geo_z, c in zip(grid_tex_z, grid_z, grid_c):\n",
        "                img, mask, sdf, deformation, v_deformed, mesh_v, mesh_f, gen_camera, img_wo_light, tex_hard_mask = G_ema.generate_3d(\n",
        "                    z=z, geo_z=geo_z, c=c, noise_mode='const',\n",
        "                    generate_no_light=True, truncation_psi=0.7, camera=camera)\n",
        "                rgb_img = img[:, :3]\n",
        "                save_img = torch.cat([rgb_img, mask.permute(0, 3, 1, 2).expand(-1, 3, -1, -1)], dim=-1).detach()\n",
        "                images_list.append(save_img.cpu().numpy())\n",
        "                mesh_v_list.extend([v.data.cpu().numpy() for v in mesh_v])\n",
        "                mesh_f_list.extend([f.data.cpu().numpy() for f in mesh_f])\n",
        "            images = np.concatenate(images_list, axis=0)\n",
        "            if save_gif_name is None:\n",
        "                save_file_name = 'fakes'\n",
        "            else:\n",
        "                save_file_name = 'fakes_%s' % (save_gif_name.split('.')[0])\n",
        "            if save_all:\n",
        "                img = save_image_grid(\n",
        "                    images, None,\n",
        "                    drange=[-1, 1], grid_size=grid_size)\n",
        "            else:\n",
        "                img = save_image_grid(\n",
        "                    images, os.path.join(\n",
        "                        run_dir,\n",
        "                        f'{save_file_name}_{cur_nimg // 1000:06d}_{i_camera:02d}.png'),\n",
        "                    drange=[-1, 1], grid_size=grid_size)\n",
        "            camera_img_list.append(img)\n",
        "        if save_gif_name is None:\n",
        "            save_gif_name = f'fakes_{cur_nimg // 1000:06d}.gif'\n",
        "        if save_all:\n",
        "            imageio.mimsave(os.path.join(run_dir, save_gif_name), camera_img_list)\n",
        "        n_shape = 10  # we only save 10 shapes to check performance\n",
        "        if cur_tick % min((image_snapshot_ticks * 20), 100) == 0:\n",
        "            save_3d_shape(mesh_v_list[:n_shape], mesh_f_list[:n_shape], run_dir, cur_nimg // 100)"
      ],
      "metadata": {
        "id": "9bT9P_JxAhA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_textured_mesh_for_inference(\n",
        "        G_ema, grid_z, grid_c, run_dir, save_mesh_dir=None,\n",
        "        c_to_compute_w_avg=None, grid_tex_z=None, use_style_mixing=False):\n",
        "    with torch.no_grad():\n",
        "        G_ema.update_w_avg(c_to_compute_w_avg)\n",
        "        save_mesh_idx = 0\n",
        "        mesh_dir = os.path.join(run_dir, save_mesh_dir)\n",
        "        os.makedirs(mesh_dir, exist_ok=True)\n",
        "        for idx in range(len(grid_z)):\n",
        "            geo_z = grid_z[idx]\n",
        "            if grid_tex_z is None:\n",
        "                tex_z = grid_z[idx]\n",
        "            else:\n",
        "                tex_z = grid_tex_z[idx]\n",
        "            generated_mesh = G_ema.generate_3d_mesh(\n",
        "                geo_z=geo_z, tex_z=tex_z, c=None, truncation_psi=0.7,\n",
        "                use_style_mixing=use_style_mixing)\n",
        "            for mesh_v, mesh_f, all_uvs, all_mesh_tex_idx, tex_map in zip(*generated_mesh):\n",
        "                savemeshtes2(\n",
        "                    mesh_v.data.cpu().numpy(),\n",
        "                    all_uvs.data.cpu().numpy(),\n",
        "                    mesh_f.data.cpu().numpy(),\n",
        "                    all_mesh_tex_idx.data.cpu().numpy(),\n",
        "                    os.path.join(mesh_dir, '%07d.obj' % (save_mesh_idx))\n",
        "                )\n",
        "                lo, hi = (-1, 1)\n",
        "                img = np.asarray(tex_map.permute(1, 2, 0).data.cpu().numpy(), dtype=np.float32)\n",
        "                img = (img - lo) * (255 / (hi - lo))\n",
        "                img = img.clip(0, 255)\n",
        "                mask = np.sum(img.astype(np.float), axis=-1, keepdims=True)\n",
        "                mask = (mask <= 3.0).astype(np.float)\n",
        "                kernel = np.ones((3, 3), 'uint8')\n",
        "                dilate_img = cv2.dilate(img, kernel, iterations=1)\n",
        "                img = img * (1 - mask) + dilate_img * mask\n",
        "                img = img.clip(0, 255).astype(np.uint8)\n",
        "                PIL.Image.fromarray(np.ascontiguousarray(img[::-1, :, :]), 'RGB').save(\n",
        "                    os.path.join(mesh_dir, '%07d.png' % (save_mesh_idx)))\n",
        "                save_mesh_idx += 1"
      ],
      "metadata": {
        "id": "Tgc4ZzNHAi9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_geo_for_inference(G_ema, run_dir):\n",
        "    import kaolin as kal\n",
        "    def normalize_and_sample_points(mesh_v, mesh_f, kal, n_sample, normalized_scale=1.0):\n",
        "        vertices = mesh_v.cuda()\n",
        "        scale = (vertices.max(dim=0)[0] - vertices.min(dim=0)[0]).max()\n",
        "        mesh_v1 = vertices / scale * normalized_scale\n",
        "        mesh_f1 = mesh_f.cuda()\n",
        "        points, _ = kal.ops.mesh.sample_points(mesh_v1.unsqueeze(dim=0), mesh_f1, n_sample)\n",
        "        return points\n",
        "\n",
        "    with torch.no_grad():\n",
        "        use_style_mixing = True\n",
        "        truncation_phi = 1.0\n",
        "        mesh_dir = os.path.join(run_dir, 'gen_geo_for_eval_phi_%.2f' % (truncation_phi))\n",
        "        surface_point_dir = os.path.join(run_dir, 'gen_geo_surface_points_for_eval_phi_%.2f' % (truncation_phi))\n",
        "        os.makedirs(mesh_dir, exist_ok=True)\n",
        "        os.makedirs(surface_point_dir, exist_ok=True)\n",
        "        n_gen = 1500 * 5  # We generate 5x of test set here\n",
        "        i_mesh = 0\n",
        "        for i_gen in tqdm(range(n_gen)):\n",
        "            geo_z = torch.randn(1, G_ema.z_dim, device=G_ema.device)\n",
        "            generated_mesh = G_ema.generate_3d_mesh(\n",
        "                geo_z=geo_z, tex_z=None, c=None, truncation_psi=truncation_phi,\n",
        "                with_texture=False, use_style_mixing=use_style_mixing)\n",
        "            for mesh_v, mesh_f in zip(*generated_mesh):\n",
        "                if mesh_v.shape[0] == 0: continue\n",
        "                save_obj(mesh_v.data.cpu().numpy(), mesh_f.data.cpu().numpy(), os.path.join(mesh_dir, '%07d.obj' % (i_mesh)))\n",
        "                points = normalize_and_sample_points(mesh_v, mesh_f, kal, n_sample=2048, normalized_scale=1.0)\n",
        "                np.savez(os.path.join(surface_point_dir, '%07d.npz' % (i_mesh)), pcd=points.data.cpu().numpy())\n",
        "                i_mesh += 1"
      ],
      "metadata": {
        "id": "LjOvw-YpCtpv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate and save results\n"
      ],
      "metadata": {
        "id": "qKw-e36NV6kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_training_set_kwargs_for_metrics(training_set_kwargs):\n",
        "    if 'add_camera_cond' in training_set_kwargs:\n",
        "        training_set_kwargs['add_camera_cond'] = True\n",
        "    return training_set_kwargs\n",
        "\n",
        "def initialize_torch_settings(random_seed, num_gpus, rank):\n",
        "    np.random.seed(random_seed * num_gpus + rank)\n",
        "    torch.manual_seed(random_seed * num_gpus + rank)\n",
        "    torch.backends.cudnn.enabled = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    conv2d_gradfix.enabled = True\n",
        "    grid_sample_gradfix.enabled = True\n",
        "\n",
        "def generate_and_save_visualizations(G_ema, device, run_dir, grid_z, grid_c, grid_tex_z=None):\n",
        "    grid_size = (5, 5)\n",
        "    n_shape = grid_size[0] * grid_size[1]\n",
        "    print('Generating... ')\n",
        "    save_visualization(G_ema, grid_z, grid_c, run_dir, 0, grid_size, 0, save_all=False, grid_tex_z=grid_tex_z)"
      ],
      "metadata": {
        "id": "Dx8RyUUZAneW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_textured_mesh_for_inference(G_ema, device, run_dir, grid_z, grid_c, grid_tex_z=None):\n",
        "    print('==> generate inference 3d shapes with texture')\n",
        "    save_textured_mesh_for_inference(\n",
        "        G_ema, grid_z, grid_c, run_dir, save_mesh_dir='texture_mesh_for_inference',\n",
        "        c_to_compute_w_avg=None, grid_tex_z=grid_tex_z\n",
        "    )\n",
        "\n",
        "def generate_interpolation_results(G_ema, run_dir):\n",
        "    print('==> generate interpolation results')\n",
        "    save_visualization_for_interpolation(G_ema, save_dir=os.path.join(run_dir, 'interpolation'))\n",
        "\n",
        "def compute_fid_scores(metrics, G_ema, training_set_kwargs, num_gpus, rank, device, run_dir, resume_pretrain):\n",
        "    print('==> compute FID scores for generation')\n",
        "    for metric in metrics:\n",
        "        training_set_kwargs = clean_training_set_kwargs_for_metrics(training_set_kwargs)\n",
        "        training_set_kwargs['split'] = 'test'\n",
        "        result_dict = metric_main.calc_metric(\n",
        "            metric=metric, G=G_ema,\n",
        "            dataset_kwargs=training_set_kwargs, num_gpus=num_gpus, rank=rank,\n",
        "            device=device\n",
        "        )\n",
        "        metric_main.report_metric(result_dict, run_dir=run_dir, snapshot_pkl=resume_pretrain)\n",
        "\n",
        "def generate_shapes_for_evaluation(G_ema, run_dir):\n",
        "    print('==> generate 7500 shapes for evaluation')\n",
        "    save_geo_for_inference(G_ema, run_dir)"
      ],
      "metadata": {
        "id": "iD1YHYP9AyvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inferencing function"
      ],
      "metadata": {
        "id": "BfMIItLCCwAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(\n",
        "        run_dir='.', training_set_kwargs={}, G_kwargs={}, D_kwargs={}, metrics=[],\n",
        "        random_seed=0, num_gpus=1, rank=0, inference_vis=False,\n",
        "        inference_to_generate_textured_mesh=False, resume_pretrain=None,\n",
        "        inference_save_interpolation=False, inference_compute_fid=False,\n",
        "        inference_generate_geo=False, **dummy_kawargs\n",
        "):\n",
        "    from torch_utils.ops import upfirdn2d, bias_act, filtered_lrelu\n",
        "\n",
        "    upfirdn2d._init()\n",
        "    bias_act._init()\n",
        "    filtered_lrelu._init()\n",
        "\n",
        "    device = torch.device('cuda', rank)\n",
        "    initialize_torch_settings(random_seed, num_gpus, rank)\n",
        "\n",
        "    common_kwargs = dict(\n",
        "        c_dim=0, img_resolution=training_set_kwargs.get('resolution', 1024), img_channels=3\n",
        "    )\n",
        "    G_kwargs['device'] = device\n",
        "\n",
        "    G = dnnlib.util.construct_class_by_name(**G_kwargs, **common_kwargs).train().requires_grad_(False).to(device)\n",
        "    G_ema = copy.deepcopy(G).eval()\n",
        "\n",
        "    if resume_pretrain is not None and rank == 0:\n",
        "        print('==> resume from pretrained path %s' % resume_pretrain)\n",
        "        model_state_dict = torch.load(resume_pretrain, map_location=device)\n",
        "        G.load_state_dict(model_state_dict['G'], strict=True)\n",
        "        G_ema.load_state_dict(model_state_dict['G_ema'], strict=True)\n",
        "\n",
        "    grid_z = torch.randn([25, G.z_dim], device=device).split(1)\n",
        "    grid_tex_z = torch.randn([25, G.z_dim], device=device).split(1)\n",
        "    grid_c = torch.ones(25, device=device).split(1)\n",
        "\n",
        "    generate_and_save_visualizations(G_ema, device, run_dir, grid_z, grid_c, grid_tex_z)\n",
        "\n",
        "    if inference_to_generate_textured_mesh:\n",
        "        generate_textured_mesh_for_inference(G_ema, device, run_dir, grid_z, grid_c, grid_tex_z)\n",
        "\n",
        "    if inference_save_interpolation:\n",
        "        generate_interpolation_results(G_ema, run_dir)\n",
        "\n",
        "    if inference_compute_fid:\n",
        "        compute_fid_scores(metrics, G_ema, training_set_kwargs, num_gpus, rank, device, run_dir, resume_pretrain)\n",
        "\n",
        "    if inference_generate_geo:\n",
        "        generate_shapes_for_evaluation(G_ema, run_dir)\n"
      ],
      "metadata": {
        "id": "L1S1dU8hRzWF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train_3d.py"
      ],
      "metadata": {
        "id": "7dn30IaVCbGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subprocess_fn(rank, c, temp_dir):\n",
        "    dnnlib.util.Logger(file_name=os.path.join(c.run_dir, 'log.txt'), file_mode='a', should_flush=True)\n",
        "\n",
        "    # Init torch.distributed.\n",
        "    if c.num_gpus > 1:\n",
        "        init_file = os.path.abspath(os.path.join(temp_dir, '.torch_distributed_init'))\n",
        "        if os.name == 'nt':\n",
        "            init_method = 'file:///' + init_file.replace('\\\\', '/')\n",
        "            torch.distributed.init_process_group(\n",
        "                backend='gloo', init_method=init_method, rank=rank, world_size=c.num_gpus)\n",
        "        else:\n",
        "            init_method = f'file://{init_file}'\n",
        "            torch.distributed.init_process_group(\n",
        "                backend='nccl', init_method=init_method, rank=rank, world_size=c.num_gpus)\n",
        "\n",
        "    # Init torch_utils.\n",
        "    sync_device = torch.device('cuda', rank) if c.num_gpus > 1 else None\n",
        "    training_stats.init_multiprocessing(rank=rank, sync_device=sync_device)\n",
        "    if rank != 0:\n",
        "        custom_ops.verbosity = 'none'\n",
        "\n",
        "    if c.inference_vis:\n",
        "        inference(rank=rank, **c)\n",
        "    # Execute training loop.\n",
        "    else:\n",
        "        training_loop_3d.training_loop(rank=rank, **c)"
      ],
      "metadata": {
        "id": "8IxpfGpsBoWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training/Inferencing Launcher"
      ],
      "metadata": {
        "id": "0_EX7QxjClMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def launch_training(c, desc, outdir, dry_run):\n",
        "    dnnlib.util.Logger(should_flush=True)\n",
        "\n",
        "    # Pick output directory.\n",
        "    prev_run_dirs = []\n",
        "    if os.path.isdir(outdir):\n",
        "        prev_run_dirs = [x for x in os.listdir(outdir) if os.path.isdir(os.path.join(outdir, x))]\n",
        "    prev_run_ids = [re.match(r'^\\d+', x) for x in prev_run_dirs]\n",
        "    prev_run_ids = [int(x.group()) for x in prev_run_ids if x is not None]\n",
        "    cur_run_id = max(prev_run_ids, default=-1) + 1\n",
        "    if c.inference_vis:\n",
        "        c.run_dir = os.path.join(outdir, 'inference')\n",
        "    else:\n",
        "        c.run_dir = os.path.join(outdir, f'{cur_run_id:05d}-{desc}')\n",
        "        assert not os.path.exists(c.run_dir)\n",
        "\n",
        "    # Create output directory.\n",
        "    print('Creating output directory...')\n",
        "    if not os.path.exists(c.run_dir):\n",
        "        os.makedirs(c.run_dir)\n",
        "    with open(os.path.join(c.run_dir, 'training_options.json'), 'wt') as f:\n",
        "        json.dump(c, f, indent=2)\n",
        "\n",
        "    # Launch processes.\n",
        "    print('Launching processes...')\n",
        "    torch.multiprocessing.set_start_method('spawn', force=True)\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        if c.num_gpus == 1:\n",
        "            subprocess_fn(rank=0, c=c, temp_dir=temp_dir)\n",
        "        else:\n",
        "            torch.multiprocessing.spawn(fn=subprocess_fn, args=(c, temp_dir), nprocs=c.num_gpus)"
      ],
      "metadata": {
        "id": "pwq6El-IBqBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initializing Arguments"
      ],
      "metadata": {
        "id": "oqAR5a8oCRbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_dataset_kwargs(data, opt=None):\n",
        "    try:\n",
        "        if opt.use_shapenet_split:\n",
        "            dataset_kwargs = dnnlib.EasyDict(\n",
        "                class_name='training.dataset.ImageFolderDataset',\n",
        "                path=data, use_labels=True, max_size=None, xflip=False,\n",
        "                resolution=opt.img_res,\n",
        "                data_camera_mode=opt.data_camera_mode,\n",
        "                add_camera_cond=opt.add_camera_cond,\n",
        "                camera_path=opt.camera_path,\n",
        "                split='test' if opt.inference_vis else 'train',\n",
        "            )\n",
        "        else:\n",
        "            dataset_kwargs = dnnlib.EasyDict(\n",
        "                class_name='training.dataset.ImageFolderDataset',\n",
        "                path=data, use_labels=True, max_size=None, xflip=False, resolution=opt.img_res,\n",
        "                data_camera_mode=opt.data_camera_mode,\n",
        "                add_camera_cond=opt.add_camera_cond,\n",
        "                camera_path=opt.camera_path,\n",
        "            )\n",
        "        dataset_obj = dnnlib.util.construct_class_by_name(**dataset_kwargs)  # Subclass of training.dataset.Dataset.\n",
        "        dataset_kwargs.camera_path = opt.camera_path\n",
        "        dataset_kwargs.resolution = dataset_obj.resolution  # Be explicit about resolution.\n",
        "        dataset_kwargs.use_labels = dataset_obj.has_labels  # Be explicit about labels.\n",
        "        dataset_kwargs.max_size = len(dataset_obj)  # Be explicit about dataset size.\n",
        "        return dataset_kwargs, dataset_obj.name\n",
        "    except IOError as err:\n",
        "        raise click.ClickException(f'--data: {err}')\n",
        "\n",
        "\n",
        "def parse_comma_separated_list(s):\n",
        "    if isinstance(s, list):\n",
        "        return s\n",
        "    if s is None or s.lower() == 'none' or s == '':\n",
        "        return []\n",
        "    return s.split(',')"
      ],
      "metadata": {
        "id": "WWoWmsXVBwuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arguments using click command and main file [from main repo]\n"
      ],
      "metadata": {
        "id": "gtaiszzfCDhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@click.command()\n",
        "# Required from StyleGAN2.\n",
        "@click.option('--outdir', help='Where to save the results', metavar='DIR', required=True)\n",
        "@click.option('--cfg', help='Base configuration', type=click.Choice(['stylegan3-t', 'stylegan3-r', 'stylegan2']), default='stylegan2')\n",
        "@click.option('--gpus', help='Number of GPUs to use', metavar='INT', type=click.IntRange(min=1), required=True)\n",
        "@click.option('--batch', help='Total batch size', metavar='INT', type=click.IntRange(min=1), required=True)\n",
        "@click.option('--gamma', help='R1 regularization weight', metavar='FLOAT', type=click.FloatRange(min=0), required=True)\n",
        "# My custom configs\n",
        "### Configs for inference\n",
        "@click.option('--resume_pretrain', help='Resume from given network pickle', metavar='[PATH|URL]', type=str)\n",
        "@click.option('--inference_vis', help='whther we run infernce', metavar='BOOL', type=bool, default=False, show_default=True)\n",
        "@click.option('--inference_to_generate_textured_mesh', help='inference to generate textured meshes', metavar='BOOL', type=bool, default=False, show_default=False)\n",
        "@click.option('--inference_save_interpolation', help='inference to generate interpolation results', metavar='BOOL', type=bool, default=False, show_default=False)\n",
        "@click.option('--inference_compute_fid', help='inference to generate interpolation results', metavar='BOOL', type=bool, default=False, show_default=False)\n",
        "@click.option('--inference_generate_geo', help='inference to generate geometry points', metavar='BOOL', type=bool, default=False, show_default=False)\n",
        "### Configs for dataset\n",
        "\n",
        "@click.option('--data', help='Path to the Training data Images', metavar='[DIR]', type=str, default='./tmp')\n",
        "@click.option('--camera_path', help='Path to the camera root', metavar='[DIR]', type=str, default='./tmp')\n",
        "@click.option('--img_res', help='The resolution of image', metavar='INT', type=click.IntRange(min=1), default=1024)\n",
        "@click.option('--data_camera_mode', help='The type of dataset we are using', type=str, default='shapenet_car', show_default=True)\n",
        "@click.option('--use_shapenet_split', help='whether use the training split or all the data for training', metavar='BOOL', type=bool, default=False, show_default=False)\n",
        "### Configs for 3D generator##########\n",
        "@click.option('--iso_surface', help='Differentiable iso-surfacing method', type=click.Choice(['dmtet', 'flexicubes']), default='dmtet')\n",
        "@click.option('--use_style_mixing', help='whether use style mixing for generation during inference', metavar='BOOL', type=bool, default=True, show_default=False)\n",
        "@click.option('--one_3d_generator', help='whether we detach the gradient for empty object', metavar='BOOL', type=bool, default=True, show_default=True)\n",
        "@click.option('--dmtet_scale', help='Scale for the dimention of dmtet', metavar='FLOAT', type=click.FloatRange(min=0, max=10.0), default=1.0, show_default=True)\n",
        "@click.option('--n_implicit_layer', help='Number of Implicit FC layer for XYZPlaneTex model', metavar='INT', type=click.IntRange(min=1), default=1)\n",
        "@click.option('--feat_channel', help='Feature channel for TORGB layer', metavar='INT', type=click.IntRange(min=0), default=16)\n",
        "@click.option('--mlp_latent_channel', help='mlp_latent_channel for XYZPlaneTex network', metavar='INT', type=click.IntRange(min=8), default=32)\n",
        "@click.option('--deformation_multiplier', help='Multiplier for the predicted deformation', metavar='FLOAT', type=click.FloatRange(min=1.0), default=1.0, required=False)\n",
        "@click.option('--tri_plane_resolution', help='The resolution for tri plane', metavar='INT', type=click.IntRange(min=1), default=256)\n",
        "@click.option('--n_views', help='number of views when training generator', metavar='INT', type=click.IntRange(min=1), default=1)\n",
        "@click.option('--use_tri_plane', help='Whether use tri plane representation', metavar='BOOL', type=bool, default=True, show_default=True)\n",
        "@click.option('--tet_res', help='Resolution for teteahedron', metavar='INT', type=click.IntRange(min=1), default=90)\n",
        "@click.option('--latent_dim', help='Dimention for latent code', metavar='INT', type=click.IntRange(min=1), default=512)\n",
        "@click.option('--geometry_type', help='The type of geometry generator', type=str, default='conv3d', show_default=True)\n",
        "@click.option('--render_type', help='Type of renderer we used', metavar='STR', type=click.Choice(['neural_render', 'spherical_gaussian']), default='neural_render', show_default=True)\n",
        "### Configs for training loss and discriminator#\n",
        "@click.option('--d_architecture', help='The architecture for discriminator', metavar='STR', type=str, default='skip', show_default=True)\n",
        "@click.option('--use_pl_length', help='whether we apply path length regularization', metavar='BOOL', type=bool, default=False, show_default=False)  # We didn't use path lenth regularzation to avoid nan error\n",
        "@click.option('--gamma_mask', help='R1 regularization weight for mask', metavar='FLOAT', type=click.FloatRange(min=0), default=0.0, required=False)\n",
        "@click.option('--d_reg_interval', help='The internal for R1 regularization', metavar='INT', type=click.IntRange(min=1), default=16)\n",
        "@click.option('--add_camera_cond', help='Whether we add camera as condition for discriminator', metavar='BOOL', type=bool, default=True, show_default=True)\n",
        "@click.option('--lambda_flexicubes_surface_reg', help='Weights for flexicubes regularization L_dev', metavar='FLOAT', type=click.FloatRange(min=0), default=0.5, required=False)\n",
        "@click.option('--lambda_flexicubes_weights_reg', help='Weights for flexicubes regularization on weights', metavar='FLOAT', type=click.FloatRange(min=0), default=0.1, required=False)\n",
        "\n",
        "## Miscs\n",
        "# Optional features.\n",
        "@click.option('--cond', help='Train conditional model', metavar='BOOL', type=bool, default=False, show_default=True)\n",
        "@click.option('--freezed', help='Freeze first layers of D', metavar='INT', type=click.IntRange(min=0), default=0, show_default=True)\n",
        "# Misc hyperparameters.\n",
        "@click.option('--batch-gpu', help='Limit batch size per GPU', metavar='INT', type=click.IntRange(min=1), default=4)\n",
        "@click.option('--cbase', help='Capacity multiplier', metavar='INT', type=click.IntRange(min=1), default=32768, show_default=True)\n",
        "@click.option('--cmax', help='Max. feature maps', metavar='INT', type=click.IntRange(min=1), default=512, show_default=True)\n",
        "@click.option('--glr', help='G learning rate  [default: varies]', metavar='FLOAT', type=click.FloatRange(min=0))\n",
        "@click.option('--dlr', help='D learning rate', metavar='FLOAT', type=click.FloatRange(min=0), default=0.002, show_default=True)\n",
        "@click.option('--map-depth', help='Mapping network depth  [default: varies]', metavar='INT', type=click.IntRange(min=1))\n",
        "@click.option('--mbstd-group', help='Minibatch std group size', metavar='INT', type=click.IntRange(min=1), default=4, show_default=True)\n",
        "# Misc settings.\n",
        "@click.option('--desc', help='String to include in result dir name', metavar='STR', type=str)\n",
        "@click.option('--metrics', help='Quality metrics', metavar='[NAME|A,B,C|none]', type=parse_comma_separated_list, default='fid50k', show_default=True)\n",
        "@click.option('--kimg', help='Total training duration', metavar='KIMG', type=click.IntRange(min=1), default=20000, show_default=True)\n",
        "@click.option('--tick', help='How often to print progress', metavar='KIMG', type=click.IntRange(min=1), default=1, show_default=True)  ##\n",
        "@click.option('--snap', help='How often to save snapshots', metavar='TICKS', type=click.IntRange(min=1), default=50, show_default=True)  ###\n",
        "@click.option('--seed', help='Random seed', metavar='INT', type=click.IntRange(min=0), default=0, show_default=True)\n",
        "@click.option('--fp32', help='Disable mixed-precision', metavar='BOOL', type=bool, default=True, show_default=True)  # Let's use fp32 all the case without clamping\n",
        "@click.option('--nobench', help='Disable cuDNN benchmarking', metavar='BOOL', type=bool, default=False, show_default=True)\n",
        "@click.option('--workers', help='DataLoader worker processes', metavar='INT', type=click.IntRange(min=0), default=3, show_default=True)\n",
        "@click.option('-n', '--dry-run', help='Print training options and exit', is_flag=True)\n",
        "\n",
        "def main(**kwargs):\n",
        "    # Initialize config.\n",
        "    print('==> start')\n",
        "    opts = dnnlib.EasyDict(kwargs)  # Command line arguments.\n",
        "    c = dnnlib.EasyDict()  # Main config dict.\n",
        "    c.G_kwargs = dnnlib.EasyDict(\n",
        "        class_name=None, z_dim=opts.latent_dim, w_dim=opts.latent_dim, mapping_kwargs=dnnlib.EasyDict())\n",
        "    c.D_kwargs = dnnlib.EasyDict(\n",
        "        class_name='training.networks_get3d.Discriminator', block_kwargs=dnnlib.EasyDict(),\n",
        "        mapping_kwargs=dnnlib.EasyDict(), epilogue_kwargs=dnnlib.EasyDict())\n",
        "    c.G_opt_kwargs = dnnlib.EasyDict(class_name='torch.optim.Adam', betas=[0, 0.99], eps=1e-8)\n",
        "    c.D_opt_kwargs = dnnlib.EasyDict(class_name='torch.optim.Adam', betas=[0, 0.99], eps=1e-8)\n",
        "    c.loss_kwargs = dnnlib.EasyDict(class_name='training.loss.StyleGAN2Loss')\n",
        "\n",
        "    c.data_loader_kwargs = dnnlib.EasyDict(pin_memory=True, prefetch_factor=2)\n",
        "    c.inference_vis = opts.inference_vis\n",
        "    # Training set.\n",
        "    if opts.inference_vis:\n",
        "        c.inference_to_generate_textured_mesh = opts.inference_to_generate_textured_mesh\n",
        "        c.inference_save_interpolation = opts.inference_save_interpolation\n",
        "        c.inference_compute_fid = opts.inference_compute_fid\n",
        "        c.inference_generate_geo = opts.inference_generate_geo\n",
        "\n",
        "    c.training_set_kwargs, dataset_name = init_dataset_kwargs(data=opts.data, opt=opts)\n",
        "    if opts.cond and not c.training_set_kwargs.use_labels:\n",
        "        raise click.ClickException('--cond=True requires labels specified in dataset.json')\n",
        "    c.training_set_kwargs.split = 'train' if opts.use_shapenet_split else 'all'\n",
        "    if opts.use_shapenet_split and opts.inference_vis:\n",
        "        c.training_set_kwargs.split = 'test'\n",
        "    c.training_set_kwargs.use_labels = opts.cond\n",
        "    c.training_set_kwargs.xflip = False\n",
        "    # Hyperparameters & settings.p\n",
        "    c.G_kwargs.iso_surface = opts.iso_surface\n",
        "    c.G_kwargs.one_3d_generator = opts.one_3d_generator\n",
        "    c.G_kwargs.n_implicit_layer = opts.n_implicit_layer\n",
        "    c.G_kwargs.deformation_multiplier = opts.deformation_multiplier\n",
        "    c.resume_pretrain = opts.resume_pretrain\n",
        "    c.D_reg_interval = opts.d_reg_interval\n",
        "    c.G_kwargs.use_style_mixing = opts.use_style_mixing\n",
        "    c.G_kwargs.dmtet_scale = opts.dmtet_scale\n",
        "    c.G_kwargs.feat_channel = opts.feat_channel\n",
        "    c.G_kwargs.mlp_latent_channel = opts.mlp_latent_channel\n",
        "    c.G_kwargs.tri_plane_resolution = opts.tri_plane_resolution\n",
        "    c.G_kwargs.n_views = opts.n_views\n",
        "\n",
        "    c.G_kwargs.render_type = opts.render_type\n",
        "    c.G_kwargs.use_tri_plane = opts.use_tri_plane\n",
        "    c.D_kwargs.data_camera_mode = opts.data_camera_mode\n",
        "    c.D_kwargs.add_camera_cond = opts.add_camera_cond\n",
        "\n",
        "    c.G_kwargs.tet_res = opts.tet_res\n",
        "\n",
        "    c.G_kwargs.geometry_type = opts.geometry_type\n",
        "    c.num_gpus = opts.gpus\n",
        "    c.batch_size = opts.batch\n",
        "    c.batch_gpu = opts.batch_gpu or opts.batch // opts.gpus\n",
        "    # c.G_kwargs.geo_pos_enc = opts.geo_pos_enc\n",
        "    c.G_kwargs.data_camera_mode = opts.data_camera_mode\n",
        "    c.G_kwargs.channel_base = c.D_kwargs.channel_base = opts.cbase\n",
        "    c.G_kwargs.channel_max = c.D_kwargs.channel_max = opts.cmax\n",
        "\n",
        "    c.G_kwargs.mapping_kwargs.num_layers = 8\n",
        "\n",
        "    c.D_kwargs.architecture = opts.d_architecture\n",
        "    c.D_kwargs.block_kwargs.freeze_layers = opts.freezed\n",
        "    c.D_kwargs.epilogue_kwargs.mbstd_group_size = opts.mbstd_group\n",
        "    c.loss_kwargs.gamma_mask = opts.gamma if opts.gamma_mask == 0.0 else opts.gamma_mask\n",
        "    c.loss_kwargs.r1_gamma = opts.gamma\n",
        "    c.loss_kwargs.lambda_flexicubes_surface_reg = opts.lambda_flexicubes_surface_reg\n",
        "    c.loss_kwargs.lambda_flexicubes_weights_reg = opts.lambda_flexicubes_weights_reg\n",
        "    c.G_opt_kwargs.lr = (0.002 if opts.cfg == 'stylegan2' else 0.0025) if opts.glr is None else opts.glr\n",
        "    c.D_opt_kwargs.lr = opts.dlr\n",
        "    c.metrics = opts.metrics\n",
        "    c.total_kimg = opts.kimg\n",
        "    c.kimg_per_tick = opts.tick\n",
        "    c.image_snapshot_ticks = c.network_snapshot_ticks = opts.snap\n",
        "    c.random_seed = c.training_set_kwargs.random_seed = opts.seed\n",
        "    c.data_loader_kwargs.num_workers = opts.workers\n",
        "    c.network_snapshot_ticks = 200\n",
        "    # Sanity checks.\n",
        "    if c.batch_size % c.num_gpus != 0:\n",
        "        raise click.ClickException('--batch must be a multiple of --gpus')\n",
        "    if c.batch_size % (c.num_gpus * c.batch_gpu) != 0:\n",
        "        raise click.ClickException('--batch must be a multiple of --gpus times --batch-gpu')\n",
        "    if c.batch_gpu < c.D_kwargs.epilogue_kwargs.mbstd_group_size:\n",
        "        raise click.ClickException('--batch-gpu cannot be smaller than --mbstd')\n",
        "    if any(not metric_main.is_valid_metric(metric) for metric in c.metrics):\n",
        "        raise click.ClickException(\n",
        "            '\\n'.join(['--metrics can only contain the following values:'] + metric_main.list_valid_metrics()))\n",
        "\n",
        "    # Base configuration.\n",
        "    c.ema_kimg = c.batch_size * 10 / 32\n",
        "    c.G_kwargs.class_name = 'training.networks_get3d.GeneratorDMTETMesh'\n",
        "    c.loss_kwargs.style_mixing_prob = 0.9  # Enable style mixing regularization.\n",
        "    c.loss_kwargs.pl_weight = 0.0  # Enable path length regularization.\n",
        "    c.G_reg_interval = 4  # Enable lazy regularization for G.\n",
        "    c.G_kwargs.fused_modconv_default = 'inference_only'  # Speed up training by using regular convolutions instead of grouped convolutions.\n",
        "    # Performance-related toggles.\n",
        "    if opts.fp32:\n",
        "        c.G_kwargs.num_fp16_res = c.D_kwargs.num_fp16_res = 0\n",
        "        c.G_kwargs.conv_clamp = c.D_kwargs.conv_clamp = None\n",
        "    if opts.nobench:\n",
        "        c.cudnn_benchmark = False\n",
        "\n",
        "    # Description string.\n",
        "    desc = f'{opts.cfg:s}-{dataset_name:s}-gpus{c.num_gpus:d}-batch{c.batch_size:d}-gamma{c.loss_kwargs.r1_gamma:g}'\n",
        "    if opts.desc is not None:\n",
        "        desc += f'-{opts.desc}'\n",
        "    # Launch.\n",
        "    print('==> launch training')\n",
        "    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)"
      ],
      "metadata": {
        "id": "infG7i5NnoKs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Invoking to run the whole code"
      ],
      "metadata": {
        "id": "LEirCq94B9h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ctx = click.Context(main)\n",
        "ctx.invoke(\n",
        "    main,\n",
        "    outdir='save_inference_results/shapenet_car',\n",
        "    gpus=1,\n",
        "    batch=4,\n",
        "    gamma=40,\n",
        "    data_camera_mode='shapenet_car',\n",
        "    dmtet_scale=1.0,\n",
        "    use_shapenet_split=True,\n",
        "    one_3d_generator=True,\n",
        "    fp32=False,\n",
        "    inference_vis=True,\n",
        "    resume_pretrain='/content/GET3D/pretrained_model/shapenet_car.pt'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3RfvklHpjLc",
        "outputId": "d206e80e-c89b-4f0e-f4de-7b6322c1f05a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> start\n",
            "==> use shapenet dataset\n",
            "==> ERROR!!!! THIS SHOULD ONLY HAPPEN WHEN USING INFERENCE\n",
            "==> use image path: ./tmp, num images: 1234\n",
            "==> launch training\n",
            "\n",
            "Training options:\n",
            "{\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks_get3d.GeneratorDMTETMesh\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 8\n",
            "    },\n",
            "    \"iso_surface\": \"dmtet\",\n",
            "    \"one_3d_generator\": true,\n",
            "    \"n_implicit_layer\": 1,\n",
            "    \"deformation_multiplier\": 1.0,\n",
            "    \"use_style_mixing\": true,\n",
            "    \"dmtet_scale\": 1.0,\n",
            "    \"feat_channel\": 16,\n",
            "    \"mlp_latent_channel\": 32,\n",
            "    \"tri_plane_resolution\": 256,\n",
            "    \"n_views\": 1,\n",
            "    \"render_type\": \"neural_render\",\n",
            "    \"use_tri_plane\": true,\n",
            "    \"tet_res\": 90,\n",
            "    \"geometry_type\": \"conv3d\",\n",
            "    \"data_camera_mode\": \"shapenet_car\",\n",
            "    \"channel_base\": 32768,\n",
            "    \"channel_max\": 512,\n",
            "    \"fused_modconv_default\": \"inference_only\"\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks_get3d.Discriminator\",\n",
            "    \"block_kwargs\": {\n",
            "      \"freeze_layers\": 0\n",
            "    },\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"data_camera_mode\": \"shapenet_car\",\n",
            "    \"add_camera_cond\": true,\n",
            "    \"channel_base\": 32768,\n",
            "    \"channel_max\": 512,\n",
            "    \"architecture\": \"skip\"\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08,\n",
            "    \"lr\": 0.002\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08,\n",
            "    \"lr\": 0.002\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"gamma_mask\": 40,\n",
            "    \"r1_gamma\": 40,\n",
            "    \"lambda_flexicubes_surface_reg\": 0.5,\n",
            "    \"lambda_flexicubes_weights_reg\": 0.1,\n",
            "    \"style_mixing_prob\": 0.9,\n",
            "    \"pl_weight\": 0.0\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"prefetch_factor\": 2,\n",
            "    \"num_workers\": 3\n",
            "  },\n",
            "  \"inference_vis\": true,\n",
            "  \"inference_to_generate_textured_mesh\": false,\n",
            "  \"inference_save_interpolation\": false,\n",
            "  \"inference_compute_fid\": false,\n",
            "  \"inference_generate_geo\": false,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"./tmp\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 1234,\n",
            "    \"xflip\": false,\n",
            "    \"resolution\": 1024,\n",
            "    \"data_camera_mode\": \"shapenet_car\",\n",
            "    \"add_camera_cond\": true,\n",
            "    \"camera_path\": \"./tmp\",\n",
            "    \"split\": \"test\",\n",
            "    \"random_seed\": 0\n",
            "  },\n",
            "  \"resume_pretrain\": \"/content/GET3D/pretrained_model/shapenet_car.pt\",\n",
            "  \"D_reg_interval\": 16,\n",
            "  \"num_gpus\": 1,\n",
            "  \"batch_size\": 4,\n",
            "  \"batch_gpu\": 4,\n",
            "  \"metrics\": [\n",
            "    \"fid50k\"\n",
            "  ],\n",
            "  \"total_kimg\": 20000,\n",
            "  \"kimg_per_tick\": 1,\n",
            "  \"image_snapshot_ticks\": 50,\n",
            "  \"network_snapshot_ticks\": 200,\n",
            "  \"random_seed\": 0,\n",
            "  \"ema_kimg\": 1.25,\n",
            "  \"G_reg_interval\": 4,\n",
            "  \"run_dir\": \"save_inference_results/shapenet_car/inference\"\n",
            "}\n",
            "\n",
            "Output directory:    save_inference_results/shapenet_car/inference\n",
            "Number of GPUs:      1\n",
            "Batch size:          4 images\n",
            "Training duration:   20000 kimg\n",
            "Dataset path:        ./tmp\n",
            "Dataset size:        1234 images\n",
            "Dataset resolution:  1024\n",
            "Dataset labels:      False\n",
            "Dataset x-flips:     False\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"filtered_lrelu_plugin\"... Done.\n",
            "==> resume from pretrained path /content/GET3D/pretrained_model/shapenet_car.pt\n",
            "==> generate \n",
            "/content/GET3D/training/networks_get3d.py:467: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  camera_theta = torch.range(0, n_camera - 1, device=self.device).unsqueeze(dim=-1) / n_camera * math.pi * 2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To delete the results"
      ],
      "metadata": {
        "id": "q2Hc32ZwB4m8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf save_inference_results/"
      ],
      "metadata": {
        "id": "aD_RZpuZqK3Z"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}